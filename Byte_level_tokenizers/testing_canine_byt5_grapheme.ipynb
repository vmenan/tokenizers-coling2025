{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c0977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,CanineTokenizer\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import grapheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3676d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(input_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as file:\n",
    "        input_texts = file.readlines()\n",
    "        input_texts = [line.strip() for line in input_texts]\n",
    "    return input_texts\n",
    "\n",
    "def get_data(path):\n",
    "    sentences_ls = {}\n",
    "    files = list_files_in_directory(path)\n",
    "    for file_path in files:\n",
    "        sentences = read_data(file_path)\n",
    "        sentences_ls.update({lang_mapping[file_path.split(\".\")[-1]]:sentences})\n",
    "    return sentences_ls\n",
    "\n",
    "def calculate_tokenizer_parity(input_texts_target,input_text_source,tokenizer):\n",
    "    tokenizer_parity_ls = []\n",
    "    for s_a,s_b in zip(input_texts_target,input_text_source):\n",
    "\n",
    "        tokenized_output_sa = tokenizer(s_a)[\"input_ids\"]\n",
    "        tokenized_output_sb = tokenizer(s_b)[\"input_ids\"]\n",
    "\n",
    "        # Calculate the compression ratio\n",
    "        premium= len(tokenized_output_sa) / len(tokenized_output_sb) if len(tokenized_output_sb) != 0 else float('inf')\n",
    "        tokenizer_parity_ls.append(premium)\n",
    "    return np.mean(tokenizer_parity_ls)\n",
    "\n",
    "def calculate_tokenizer_parity_ours(input_texts_target,input_text_source):\n",
    "    tokenizer_parity_ls = []\n",
    "    for s_a,s_b in zip(input_texts_target,input_text_source):\n",
    "\n",
    "        tokenized_output_sa = list(grapheme.graphemes(s_a))\n",
    "        tokenized_output_sb = list(grapheme.graphemes(s_b))\n",
    "\n",
    "        # Calculate the compression ratio\n",
    "        premium= len(tokenized_output_sa) / len(tokenized_output_sb) if len(tokenized_output_sb) != 0 else float('inf')\n",
    "        tokenizer_parity_ls.append(premium)\n",
    "    return np.mean(tokenizer_parity_ls)\n",
    "\n",
    "def evaluate_compression_ratio(input_texts,tokenizer):\n",
    "    \"\"\"\n",
    "    Evaluates the compression ratio of a tokenizer.\n",
    "\n",
    "    Args:\n",
    "    tokenizer (PreTrainedTokenizer): The tokenizer to evaluate.\n",
    "    text (str): The original text data.\n",
    "\n",
    "    Returns:\n",
    "    float: The compression ratio.\n",
    "    \"\"\"\n",
    "    compression_ratio_ls = []\n",
    "    for text in input_texts:\n",
    "        # Calculate the size of the original text in characters\n",
    "        original_size = len(text)\n",
    "\n",
    "        tokenized_output = tokenizer(text)[\"input_ids\"]\n",
    "\n",
    "        # Calculate the size of the tokenized data in tokens\n",
    "        tokenized_size = len(tokenized_output)\n",
    "\n",
    "        # Calculate the compression ratio\n",
    "        compression_ratio = original_size / tokenized_size if tokenized_size != 0 else float('inf')\n",
    "        compression_ratio_ls.append(compression_ratio)\n",
    "    \n",
    "    return np.mean(compression_ratio_ls)\n",
    "\n",
    "def evaluate_compression_ratio_ours(input_texts):\n",
    "    \"\"\"\n",
    "    Evaluates the compression ratio of a tokenizer.\n",
    "\n",
    "    Args:\n",
    "    tokenizer (PreTrainedTokenizer): The tokenizer to evaluate.\n",
    "    text (str): The original text data.\n",
    "\n",
    "    Returns:\n",
    "    float: The compression ratio.\n",
    "    \"\"\"\n",
    "    compression_ratio_ls = []\n",
    "    for text in input_texts:\n",
    "        # Calculate the size of the original text in characters\n",
    "        original_size = len(text)\n",
    "\n",
    "        tokenized_output = list(grapheme.graphemes(text))\n",
    "\n",
    "        # Calculate the size of the tokenized data in tokens\n",
    "        tokenized_size = len(tokenized_output)\n",
    "\n",
    "        # Calculate the compression ratio\n",
    "        compression_ratio = original_size / tokenized_size if tokenized_size != 0 else float('inf')\n",
    "        compression_ratio_ls.append(compression_ratio)\n",
    "    \n",
    "    return np.mean(compression_ratio_ls)\n",
    "\n",
    "def list_files_in_directory(path):\n",
    "    \"\"\"\n",
    "    Returns a list of files in the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    path (str): The directory path where to list the files.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of file names in the directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # List only files, not directories\n",
    "        files = [os.path.join(path,f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "        return files\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The directory '{path}' does not exist.\")\n",
    "        return []\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied for accessing the directory '{path}'.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af4a94e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = [(\"google/canine-c\",CanineTokenizer),\n",
    "             (\"google/byt5-base\",AutoTokenizer)\n",
    "             ]\n",
    "lang_mapping = {\n",
    "    \"eng_Latn\": \"english\",\n",
    "    \"hin_Deva\" : \"hindi\",\n",
    "    \"sin_Sinh\": \"sinhala\",\n",
    "    \"tam_Taml\": \"tamil\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64f960e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/canine-c\n",
      "False\n",
      "google/byt5-base\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "data_tokenizer_parity = {\n",
    "    \"name\":[],\n",
    "    \"english\":[],\n",
    "    \"tamil\":[],\n",
    "    \"sinhala\":[],\n",
    "    \"hindi\":[]\n",
    "}\n",
    "\n",
    "data_compression_ratio = {\n",
    "    \"name\":[],\n",
    "    \"english\":[],\n",
    "    \"tamil\":[],\n",
    "    \"sinhala\":[],\n",
    "    \"hindi\":[]\n",
    "}\n",
    "\n",
    "for name,t in tokenizers:\n",
    "    print(name)\n",
    "    tokenizer = t.from_pretrained(name,trust_remote_code=True)\n",
    "    sentences_ls = get_data(\"./flores/\")\n",
    "    data_compression_ratio[\"name\"].append(name)\n",
    "    data_tokenizer_parity[\"name\"].append(name)\n",
    "    print(tokenizer.is_fast)\n",
    "    for lang,sentences in sentences_ls.items():\n",
    "        comp_ratio = evaluate_compression_ratio(sentences,tokenizer)\n",
    "        parity = calculate_tokenizer_parity(sentences,sentences_ls[\"english\"],tokenizer)\n",
    "        data_compression_ratio[lang].append(round(comp_ratio,2))\n",
    "        data_tokenizer_parity[lang].append(round(parity,2))\n",
    "\n",
    "#appending our method as well\n",
    "sentences_ls = get_data(\"./flores/\")\n",
    "data_compression_ratio[\"name\"].append(\"Ours\")\n",
    "data_tokenizer_parity[\"name\"].append(\"Ours\")\n",
    "for lang,sentences in sentences_ls.items():\n",
    "    comp_ratio = evaluate_compression_ratio_ours(sentences)\n",
    "    parity = calculate_tokenizer_parity_ours(sentences,sentences_ls[\"english\"])\n",
    "    data_compression_ratio[lang].append(round(comp_ratio,2))\n",
    "    data_tokenizer_parity[lang].append(round(parity,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab183f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': ['google/canine-c', 'google/byt5-base', 'Ours'],\n",
       " 'english': [np.float64(0.98), np.float64(0.99), np.float64(1.0)],\n",
       " 'tamil': [np.float64(0.99), np.float64(0.37), np.float64(1.55)],\n",
       " 'sinhala': [np.float64(0.98), np.float64(0.38), np.float64(1.41)],\n",
       " 'hindi': [np.float64(0.98), np.float64(0.39), np.float64(1.45)]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_compression_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5632c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compression = pd.DataFrame(data=data_compression_ratio)\n",
    "df_parity = pd.DataFrame(data=data_tokenizer_parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbd191e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>english</th>\n",
       "      <th>tamil</th>\n",
       "      <th>sinhala</th>\n",
       "      <th>hindi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google/canine-c</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google/byt5-base</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ours</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name  english  tamil  sinhala  hindi\n",
       "0   google/canine-c     0.98   0.99     0.98   0.98\n",
       "1  google/byt5-base     0.99   0.37     0.38   0.39\n",
       "2              Ours     1.00   1.55     1.41   1.45"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compression.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c258465",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compression.to_csv(\"./results/compression_ratio_byte_level.csv\")\n",
    "df_parity.to_csv(\"./results/parity_byte_level.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3965460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

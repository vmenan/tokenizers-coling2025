{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a47e0415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedTokenizer\n",
    "import tiktoken\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import regex as re\n",
    "from huggingface_hub import notebook_login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d0148e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = [\n",
    "#     (\"gpt2\"),\n",
    "#     (\"meta-llama/Llama-2-7b-chat-hf\"),\n",
    "# #     (\"meta-llama/Meta-Llama-3.1-8B-Instruct\"),\n",
    "#     (\"google/flan-t5-base\"),\n",
    "#     (\"google/gemma-2-2b-it\")\n",
    "    (\"CohereForAI/aya-101\"),\n",
    "#     (\"bigscience/bloom-560m\"),\n",
    "#     (\"bigscience/bloomz\"),\n",
    "#     (\"abhinand/tamil-llama-7b-instruct-v0.1\"),\n",
    "#     (\"aisingapore/sea-lion-7b-instruct\"),\n",
    "#     (\"google-bert/bert-base-uncased\"),\n",
    "    (\"google-bert/bert-base-multilingual-uncased\"),\n",
    "#     (\"google-t5/t5-base\"),\n",
    "    (\"google/mt5-base\"),\n",
    "    (\"facebook/mbart-large-50\"),\n",
    "    (\"facebook/nllb-200-distilled-600M\"),\n",
    "\n",
    "]\n",
    "lang_mapping = {\n",
    "    \"eng_Latn\": \"english\",\n",
    "    \"hin_Deva\" : \"hindi\",\n",
    "    \"sin_Sinh\": \"sinhala\",\n",
    "    \"tam_Taml\": \"tamil\"\n",
    "}\n",
    "tokenizers_openai = [\n",
    "    (\"gpt-4o-\",\"o200k_base\"),\n",
    "    (\"gpt-4-\",\"cl100k_base\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "500ac6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(input_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as file:\n",
    "        input_texts = file.readlines()\n",
    "        input_texts = [line.strip() for line in input_texts]\n",
    "    return input_texts\n",
    "\n",
    "def extract_text_chunks(text, index_pairs):\n",
    "    \"\"\"\n",
    "    Extract text chunks from the input text based on the given index pairs.\n",
    "\n",
    "    :param text: The input string from which to extract chunks.\n",
    "    :param index_pairs: A list of tuples, where each tuple contains a pair of start and end indices.\n",
    "    :return: A list of extracted text chunks.\n",
    "    \"\"\"\n",
    "    text_chunks = [text[start:end] for start, end in index_pairs]\n",
    "    return text_chunks\n",
    "\n",
    "def list_files_in_directory(path):\n",
    "    \"\"\"\n",
    "    Returns a list of files in the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    path (str): The directory path where to list the files.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of file names in the directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # List only files, not directories\n",
    "        files = [os.path.join(path,f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "        return files\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The directory '{path}' does not exist.\")\n",
    "        return []\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied for accessing the directory '{path}'.\")\n",
    "        return []\n",
    "\n",
    "def get_data(path):\n",
    "    sentences_ls = {}\n",
    "    files = list_files_in_directory(path)\n",
    "    for file_path in files:\n",
    "        sentences = read_data(file_path)\n",
    "        sentences_ls.update({lang_mapping[file_path.split(\".\")[-1]]:sentences})\n",
    "    return sentences_ls\n",
    "    \n",
    "def set_unknown_token(tokenizer: PreTrainedTokenizer, unknown_token: str = '<unk>'):\n",
    "    \"\"\"\n",
    "    Checks if the given tokenizer has None for the unknown token.\n",
    "    If it does, sets the unknown token and its ID.\n",
    "\n",
    "    Args:\n",
    "    tokenizer (PreTrainedTokenizer): The tokenizer to check and update.\n",
    "    unknown_token (str): The token to use as the unknown token. Default is '<unk>'.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if tokenizer.unk_token is None:\n",
    "        # Set the unknown token\n",
    "        tokenizer.unk_token = unknown_token\n",
    "        \n",
    "        # Add the unknown token to the vocabulary if it's not already there\n",
    "        if unknown_token not in tokenizer.vocab:\n",
    "            tokenizer.add_tokens([unknown_token])\n",
    "        \n",
    "        # Set the unknown token ID\n",
    "        tokenizer.unk_token_id = tokenizer.convert_tokens_to_ids(unknown_token)\n",
    "        \n",
    "        print(f\"Unknown token set to: {tokenizer.unk_token}\")\n",
    "        print(f\"Unknown token ID set to: {tokenizer.unk_token_id}\")\n",
    "    else:\n",
    "        print(f\"Tokenizer already has an unknown token: {tokenizer.unk_token}\")\n",
    "        print(f\"Unknown token ID: {tokenizer.unk_token_id}\")\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def fertility(input_texts,tokenizer):\n",
    "    text_fertility = []\n",
    "    for text in input_texts:\n",
    "        tokenized_len = len(tokenizer.tokenize(text))\n",
    "        word_count = len(text.split())\n",
    "        text_fertility.append(tokenized_len/word_count)\n",
    "    return np.mean(text_fertility)\n",
    "        \n",
    "def proportion_of_continued_words(input_texts,tokenizer): \n",
    "    continued_words = []\n",
    "    for text in input_texts:\n",
    "        words = text.split()\n",
    "        words_count = len(words)\n",
    "        continued_count = 0\n",
    "        for word in words:\n",
    "            tokenized_word = tokenizer.tokenize(word)\n",
    "            if len(tokenized_word)>1:\n",
    "                continued_count+=1\n",
    "        continued_words.append((continued_count/words_count))\n",
    "    return np.mean(continued_words)\n",
    "\n",
    "def unkown_rate(input_texts,tokenizer):\n",
    "    unknw_rate = []\n",
    "    for text in input_texts:\n",
    "        tokenized_words = tokenizer.tokenize(text)\n",
    "        tokenized_len = len(tokenized_words)\n",
    "        unkn_token_count = len(list(filter(lambda x:x==tokenizer.unk_token,tokenized_words )))\n",
    "        unknw_rate.append(unkn_token_count/tokenized_len)\n",
    "    return np.mean(unknw_rate)   \n",
    "\n",
    "\n",
    "def closeness(input_texts,tokenizer):\n",
    "    closeness = []\n",
    "    for text in input_texts:\n",
    "        tokenized_len = len(tokenizer.tokenize(text))\n",
    "        character_count = len(text)\n",
    "        closeness.append(tokenized_len/character_count)\n",
    "    return np.mean(closeness)\n",
    "\n",
    "def evaluate_compression_ratio(input_texts,pre_tokenizer):\n",
    "    \"\"\"\n",
    "    Evaluates the compression ratio of a tokenizer.\n",
    "\n",
    "    Args:\n",
    "    tokenizer (PreTrainedTokenizer): The tokenizer to evaluate.\n",
    "    text (str): The original text data.\n",
    "\n",
    "    Returns:\n",
    "    float: The compression ratio.\n",
    "    \"\"\"\n",
    "    compression_ratio_ls = []\n",
    "    for text in input_texts:\n",
    "        # Calculate the size of the original text in characters\n",
    "        original_size = len(text)\n",
    "\n",
    "        pretokenized_output = pre_tokenizer.pre_tokenize_str(text)\n",
    "        index_pairs = [ index_pair for pretokens,index_pair in pretokenized_output]\n",
    "        pretokens = extract_text_chunks(text, index_pairs)\n",
    "\n",
    "        # Calculate the size of the tokenized data in tokens\n",
    "        tokenized_size = len(pretokens)\n",
    "\n",
    "        # Calculate the compression ratio\n",
    "        compression_ratio = original_size / tokenized_size if tokenized_size != 0 else float('inf')\n",
    "        compression_ratio_ls.append(compression_ratio)\n",
    "    \n",
    "    return np.mean(compression_ratio_ls)\n",
    "\n",
    "def evaluate_compression_ratio_openai(input_texts,GPT4_SPLIT_PATTERN):\n",
    "    \"\"\"\n",
    "    Evaluates the compression ratio of a tokenizer.\n",
    "\n",
    "    Args:\n",
    "    tokenizer (PreTrainedTokenizer): The tokenizer to evaluate.\n",
    "    text (str): The original text data.\n",
    "\n",
    "    Returns:\n",
    "    float: The compression ratio.\n",
    "    \"\"\"\n",
    "    compression_ratio_ls = []\n",
    "    for text in input_texts:\n",
    "        # Calculate the size of the original text in characters\n",
    "        original_size = len(text)\n",
    "\n",
    "        # Tokenize the text\n",
    "        pretokens = re.findall(GPT4_SPLIT_PATTERN, text)\n",
    "\n",
    "        # Calculate the size of the tokenized data in tokens\n",
    "        tokenized_size = len(pretokens)\n",
    "\n",
    "        # Calculate the compression ratio\n",
    "        compression_ratio = original_size / tokenized_size if tokenized_size != 0 else float('inf')\n",
    "        compression_ratio_ls.append(compression_ratio)\n",
    "    \n",
    "    return np.mean(compression_ratio_ls)\n",
    "\n",
    "def evaluate_context_window(input_texts,pre_tokenizer):\n",
    "    context_window_ls = []\n",
    "    for text in input_texts:\n",
    "        pretokenized_output = pre_tokenizer.pre_tokenize_str(text)\n",
    "        index_pairs = [ index_pair for pretokens,index_pair in pretokenized_output]\n",
    "        pretokens = extract_text_chunks(text, index_pairs)\n",
    "        context_window_ls.append(len(pretokens))\n",
    "    return np.mean(context_window_ls)\n",
    "\n",
    "def evaluate_context_window_openai(input_texts,GPT4_SPLIT_PATTERN):\n",
    "    context_window_ls = []\n",
    "    for text in input_texts:\n",
    "        context_window_ls.append(len(re.findall(GPT4_SPLIT_PATTERN, text)))\n",
    "    return np.mean(context_window_ls)\n",
    "\n",
    "def calculate_tokenizer_parity(input_texts_target,input_text_source,pre_tokenizer):\n",
    "    tokenizer_parity_ls = []\n",
    "    for s_a,s_b in zip(input_texts_target,input_text_source):\n",
    "\n",
    "        pretokenized_output_sa = pre_tokenizer.pre_tokenize_str(s_a)\n",
    "        pretokenized_output_sb = pre_tokenizer.pre_tokenize_str(s_b)\n",
    "\n",
    "        index_pairs_sa = [ index_pair for pretokens,index_pair in pretokenized_output_sa]\n",
    "        index_pairs_sb = [ index_pair for pretokens,index_pair in pretokenized_output_sb]\n",
    "\n",
    "        pretokens_sa = extract_text_chunks(s_a, index_pairs_sa)\n",
    "        pretokens_sb = extract_text_chunks(s_b, index_pairs_sb)\n",
    "\n",
    "        # Calculate the compression ratio\n",
    "        premium= len(pretokens_sa) / len(pretokens_sb) if len(pretokens_sb) != 0 else float('inf')\n",
    "        tokenizer_parity_ls.append(premium)\n",
    "    return np.mean(tokenizer_parity_ls) \n",
    "\n",
    "def calculate_tokenizer_openai(input_texts_target,input_text_source,GPT4_SPLIT_PATTERN):\n",
    "    tokenizer_parity_ls = []\n",
    "    for s_a,s_b in zip(input_texts_target,input_text_source):\n",
    "\n",
    "        pretokens_sa = re.findall(GPT4_SPLIT_PATTERN, s_a)\n",
    "        pretokens_sb = re.findall(GPT4_SPLIT_PATTERN, s_b)\n",
    "\n",
    "        # Calculate the compression ratio\n",
    "        premium= len(pretokens_sa) / len(pretokens_sb) if len(pretokens_sb) != 0 else float('inf')\n",
    "        tokenizer_parity_ls.append(premium)\n",
    "    return np.mean(tokenizer_parity_ls) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c09e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CohereForAI/aya-101\n",
      "True\n",
      "google-bert/bert-base-multilingual-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\My Studies\\Research\\Tokenization\\LAT\\Results\\venve\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "google/mt5-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "D:\\My Studies\\Research\\Tokenization\\LAT\\Results\\venve\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "facebook/mbart-large-50\n",
      "True\n",
      "facebook/nllb-200-distilled-600M\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "data_tokenizer_parity = {\n",
    "    \"name\":[],\n",
    "    \"english\":[],\n",
    "    \"tamil\":[],\n",
    "    \"sinhala\":[],\n",
    "    \"hindi\":[]\n",
    "}\n",
    "\n",
    "data_compression_ratio = {\n",
    "    \"name\":[],\n",
    "    \"english\":[],\n",
    "    \"tamil\":[],\n",
    "    \"sinhala\":[],\n",
    "    \"hindi\":[]\n",
    "}\n",
    "for name in tokenizers:\n",
    "    print(name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name,trust_remote_code=True)\n",
    "    sentences_ls = get_data(\"./flores/\")\n",
    "    data_compression_ratio[\"name\"].append(name)\n",
    "    data_tokenizer_parity[\"name\"].append(name)\n",
    "    print(tokenizer.is_fast)\n",
    "    pre_tokenizer = tokenizer.backend_tokenizer.pre_tokenizer\n",
    "    for lang,sentences in sentences_ls.items():\n",
    "        comp_ratio = evaluate_compression_ratio(sentences,pre_tokenizer)\n",
    "        parity = calculate_tokenizer_parity(sentences,sentences_ls[\"english\"],pre_tokenizer)\n",
    "        data_compression_ratio[lang].append(round(comp_ratio,2))\n",
    "        data_tokenizer_parity[lang].append(round(parity,2))\n",
    "        pre_tokenizer = None\n",
    "\n",
    "# for name,path in tokenizers_openai:\n",
    "#     print(name)\n",
    "#     tokenizer = tiktoken.encoding_for_model(name)\n",
    "#     sentences_ls = get_data(\"./flores/\")\n",
    "#     data_compression_ratio[\"name\"].append(name)\n",
    "#     data_context_window[\"name\"].append(name)\n",
    "#     for lang,sentences in sentences_ls:\n",
    "#         comp_ratio = evaluate_compression_ratio_openai(sentences,tokenizer)\n",
    "#         context_window_len = evaluate_context_window_openai(sentences,tokenizer)\n",
    "#         data_compression_ratio[lang].append(comp_ratio)\n",
    "#         data_context_window[lang].append(context_window_len)\n",
    "\n",
    "# GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "# data_compression_ratio[\"name\"].append(\"GPT4\")\n",
    "# data_tokenizer_parity[\"name\"].append(\"GPT4\")\n",
    "# sentences_ls = get_data(\"./flores/\")\n",
    "# for lang,sentences in sentences_ls.items():\n",
    "#     comp_ratio = evaluate_compression_ratio_openai(sentences,GPT4_SPLIT_PATTERN)\n",
    "#     parity = calculate_tokenizer_openai(sentences,sentences_ls[\"english\"],GPT4_SPLIT_PATTERN)\n",
    "#     data_compression_ratio[lang].append(round(comp_ratio,2))\n",
    "#     data_tokenizer_parity[lang].append(round(parity,2))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a286bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_compression_ratio[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75c120b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compression = pd.DataFrame(data=data_compression_ratio)\n",
    "df_parity = pd.DataFrame(data=data_tokenizer_parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65aa889f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>english</th>\n",
       "      <th>tamil</th>\n",
       "      <th>sinhala</th>\n",
       "      <th>hindi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CohereForAI/aya-101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google-bert/bert-base-multilingual-uncased</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google/mt5-base</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>facebook/mbart-large-50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>facebook/nllb-200-distilled-600M</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name  english  tamil  sinhala  hindi\n",
       "0                         CohereForAI/aya-101      1.0   0.78     0.96   1.18\n",
       "1  google-bert/bert-base-multilingual-uncased      1.0   0.80     0.93   1.13\n",
       "2                             google/mt5-base      1.0   0.78     0.96   1.18\n",
       "3                     facebook/mbart-large-50      1.0   0.78     0.96   1.18\n",
       "4            facebook/nllb-200-distilled-600M      1.0   0.78     0.96   1.18"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parity.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38ec09fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compression.to_csv(\"./results/compression_ratio_multilingual_models.csv\")\n",
    "df_parity.to_csv(\"./results/parity_multilingual_models.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "960d1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"CohereForAI/aya-101\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a53ae7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31114, 71105, 51190, 4858, 30038, 1]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"හෙලෝ වර්ල්ඩ්\")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d29bc4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'හෙලෝ වර්ල්ඩ්</s>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(\"හෙලෝ වර්ල්ඩ්\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80278e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "හෙ\n",
      "ලෝ\n",
      "වර්\n",
      "ල්\n",
      "ඩ්\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "for token in tokenizer(\"හෙලෝ වර්ල්ඩ්\")[\"input_ids\"]:\n",
    "    print(tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8004a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"හෙලෝ වර්ල්ඩ්!\"\n",
    "pre_tokenizer = tokenizer.backend_tokenizer.pre_tokenizer\n",
    "pretokenized_output = pre_tokenizer.pre_tokenize_str(sentence)\n",
    "index_pairs = [ index_pair for pretokens,index_pair in pretokenized_output]\n",
    "pretokens = extract_text_chunks(sentence, index_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45a8668e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['හෙලෝ', ' වර්ල්ඩ්!']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b596e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c691bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ක්‍රීඩා\n",
    "\n",
    "&#x0D9A;&#x0DCA;&#x200D;&#x0DBB;&#x0DD3;&#x0DA9;&#x0DCF;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ffcd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
